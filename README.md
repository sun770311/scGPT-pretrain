# scGPT-pretrain
Hannah Sun

Mentor: Dr. Guan Tao Zheng

MobiDrop (Zhejiang) Co., Ltd.

## Large Language Models
Generative artificial intelligence (GenAI) is a branch of artificial intelligence that creates real-world text, images, music, and other forms of media.

A large language model (LLM), such as GPT (Generative Pretrained Transformer), is a type of generative artificial intelligence model. LLM learns from extensive data sets to process and generate natural language for tasks such as translation and conversation.

## LLMs and single-cell data
An analogy can be drawn between language and cell biology. Words form sentences, similar to genes building cells. Therefore, large language models can be adapted to single-cell RNA sequencing studies.

However, current machine learning-based methods in single-cell research are quite fragmented and have limited data sets. To overcome this limitation, the project aims to develop a single-cell-based LLM. The model is pre-trained on vast amounts of data, and then fine-tuned and tested according to different analysis tasks.

Drawing on existing foundational model scGPT, we pretrained an LLM on 300,000 human blood cells, minimizing loss between real and predicted gene expression levels. This versatile model hopes to improve the accuracy and efficiency of single-cell analysis, thereby accelerating discovery and innovation in biology.

## References
https://github.com/bowang-lab/scGPT/tree/main
https://www.nature.com/articles/s41592-024-02201-0
https://chanzuckerberg.github.io/cellxgene-census/



